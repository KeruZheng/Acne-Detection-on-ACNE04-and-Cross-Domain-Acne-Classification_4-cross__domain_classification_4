{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select random 20 sample from dermnet\n",
    "### prepare pesudo test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo test list saved to /data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_test_list.txt\n",
      "Acne: 10, Non-Acne: 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# === é…ç½® ===\n",
    "train_root = \"/data_lg/keru/project/part2/DermNet/train\"\n",
    "output_txt = \"/data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_test_list.txt\"\n",
    "\n",
    "num_total = 20      # æ€»æ•°\n",
    "num_acne = 10        # Acne æ ·æœ¬æ•°\n",
    "num_non_acne = num_total - num_acne\n",
    "\n",
    "# === æ‰¾åˆ°æ‰€æœ‰å­æ–‡ä»¶å¤¹ ===\n",
    "all_classes = [\n",
    "    d for d in os.listdir(train_root)\n",
    "    if os.path.isdir(os.path.join(train_root, d))\n",
    "]\n",
    "\n",
    "# Acne ç±»å’Œ Non-Acne ç±»åˆ†å¼€\n",
    "acne_folders = [d for d in all_classes if \"acne\" in d.lower()]\n",
    "non_acne_folders = [d for d in all_classes if d not in acne_folders]\n",
    "\n",
    "assert acne_folders, \"æ²¡æœ‰æ‰¾åˆ°åŒ…å« 'acne' çš„æ–‡ä»¶å¤¹ï¼\"\n",
    "\n",
    "# === éšæœºé€‰ Acne æ ·æœ¬ ===\n",
    "acne_samples = []\n",
    "for folder in acne_folders:\n",
    "    folder_path = os.path.join(train_root, folder)\n",
    "    imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    acne_samples.extend(imgs)\n",
    "pseudo_acne = random.sample(acne_samples, num_acne)\n",
    "\n",
    "# === éšæœºé€‰ Non-Acne æ ·æœ¬ï¼ˆä¿è¯å°½é‡ä¸åŒæ–‡ä»¶å¤¹ï¼‰===\n",
    "pseudo_non_acne = []\n",
    "selected_folders = random.sample(non_acne_folders, min(num_non_acne, len(non_acne_folders)))\n",
    "for folder in selected_folders:\n",
    "    folder_path = os.path.join(train_root, folder)\n",
    "    imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    if imgs:\n",
    "        pseudo_non_acne.append(random.choice(imgs))\n",
    "\n",
    "# è‹¥æ–‡ä»¶å¤¹ä¸å¤Ÿï¼Œç»§ç»­è¡¥\n",
    "while len(pseudo_non_acne) < num_non_acne:\n",
    "    folder = random.choice(non_acne_folders)\n",
    "    folder_path = os.path.join(train_root, folder)\n",
    "    imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    if imgs:\n",
    "        pseudo_non_acne.append(random.choice(imgs))\n",
    "\n",
    "# === åˆå¹¶ & ä¿å­˜ txt ===\n",
    "with open(output_txt, \"w\") as f:\n",
    "    for p in pseudo_acne:\n",
    "        f.write(f\"{p}\\t1\\n\")\n",
    "    for p in pseudo_non_acne:\n",
    "        f.write(f\"{p}\\t0\\n\")\n",
    "\n",
    "print(f\"Pseudo test list saved to {output_txt}\")\n",
    "print(f\"Acne: {len(pseudo_acne)}, Non-Acne: {len(pseudo_non_acne)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_ft_8/keru/conda/envs/yolov5/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data_ft_8/keru/conda/envs/yolov5/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 16.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Grad-CAM saved to /data_lg/keru/project/part2/DermNet/pseudo_eval/gradcam_pseudo\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "AUROC: 0.1300\n",
      "ğŸ“„ Metrics and prediction records saved to: /data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_eval_metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/data_ft_8/keru/conda/envs/yolov5/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import json\n",
    "\n",
    "# === é…ç½® ===\n",
    "pseudo_txt = \"/data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_test_list.txt\"\n",
    "model_path = \"/data_lg/keru/project/part2/outputs/best_resnet50.pt\"\n",
    "gradcam_dir = \"/data_lg/keru/project/part2/DermNet/pseudo_eval/gradcam_pseudo\"\n",
    "\n",
    "os.makedirs(gradcam_dir, exist_ok=True)\n",
    "\n",
    "# === åŠ è½½æ¨¡å‹ ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === é¢„å¤„ç† ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# === è¯»å–ä¼ªæµ‹è¯•é›†åˆ—è¡¨ ===\n",
    "samples = []\n",
    "labels = []\n",
    "with open(pseudo_txt) as f:\n",
    "    for line in f:\n",
    "        path, lbl = line.strip().split(\"\\t\")\n",
    "        samples.append(path)\n",
    "        labels.append(int(lbl))\n",
    "\n",
    "# === Grad-CAM è®¾ç½® ===\n",
    "target_layer = model.layer4[-1]\n",
    "cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "# === æ¨ç† ===\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "for img_path, true_label in tqdm(zip(samples, labels), total=len(samples)):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        prob = torch.softmax(output, dim=1)[0, 1].item()\n",
    "        pred = output.argmax(dim=1).cpu().item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_probs.append(prob)\n",
    "\n",
    "    # === Grad-CAM å¯è§†åŒ– ===\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(1)])[0, :]\n",
    "    rgb_img = transform(img).permute(1, 2, 0).cpu().numpy()\n",
    "    rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    out_name = os.path.basename(img_path).split(\".\")[0] + \"_gradcam.jpg\"\n",
    "    out_path = os.path.join(gradcam_dir, out_name)\n",
    "    Image.fromarray(cam_image).save(out_path)\n",
    "\n",
    "print(f\"âœ… Grad-CAM saved to {gradcam_dir}\")\n",
    "\n",
    "# === è®¡ç®—æŒ‡æ ‡ ===\n",
    "acc = accuracy_score(labels, all_preds)\n",
    "prec = precision_score(labels, all_preds)\n",
    "rec = recall_score(labels, all_preds)\n",
    "f1 = f1_score(labels, all_preds)\n",
    "try:\n",
    "    auc = roc_auc_score(labels, all_probs)\n",
    "except:\n",
    "    auc = float('nan')\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUROC: {auc:.4f}\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "# === ä¿å­˜è¯¦ç»†é¢„æµ‹ä¿¡æ¯å’Œæ•´ä½“è¯„ä¼°æŒ‡æ ‡ ===\n",
    "results = []\n",
    "for path, true_label, pred, prob in zip(samples, labels, all_preds, all_probs):\n",
    "    results.append({\n",
    "        \"image\": path,\n",
    "        \"true_label\": true_label,\n",
    "        \"pred_label\": pred,\n",
    "        \"probability\": round(prob, 4),\n",
    "        \"correct\": int(pred == true_label)\n",
    "    })\n",
    "\n",
    "metrics = {\n",
    "    \"results\": results,\n",
    "    \"summary\": {\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"precision\": round(prec, 4),\n",
    "        \"recall\": round(rec, 4),\n",
    "        \"f1_score\": round(f1, 4),\n",
    "        \"auroc\": round(auc, 4)\n",
    "    }\n",
    "}\n",
    "\n",
    "save_path = \"/data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_eval_metrics.json\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"ğŸ“„ Metrics and prediction records saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Grad-CAM saved to: /data_lg/keru/project/part2/DermNet/pseudo_eval/gradcam_v2_weight\n",
      "\n",
      "ğŸ¯ Evaluation Metrics:\n",
      "Accuracy:  0.5000\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n",
      "AUROC:     0.4400\n",
      "\n",
      "ğŸ“Š Metrics saved to: /data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_eval_metrics.json\n",
      "ğŸ“ Per-image results saved to: /data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_eval_records.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/data_ft_8/keru/conda/envs/yolov5/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import json\n",
    "\n",
    "# === é…ç½® ===\n",
    "pseudo_txt = \"/data_lg/keru/project/part2/DermNet/pseudo_eval/pseudo_test_list.txt\"\n",
    "model_path = \"/data_lg/keru/project/part2/outputs_v2_weight/best_facenet_v2.pt\"  # â† æ”¹ä¸ºä½  v2/v3 æ¨¡å‹è·¯å¾„\n",
    "gradcam_dir = \"/data_lg/keru/project/part2/DermNet/pseudo_eval/gradcam_v2_weight\"\n",
    "\n",
    "os.makedirs(gradcam_dir, exist_ok=True)\n",
    "\n",
    "# === æ¨¡å‹åŠ è½½ï¼ˆå¤„ç† module.ï¼‰ ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=2)\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === é¢„å¤„ç† ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# === è¯»å–ä¼ªæµ‹è¯•é›†åˆ—è¡¨ ===\n",
    "samples, labels = [], []\n",
    "with open(pseudo_txt) as f:\n",
    "    for line in f:\n",
    "        path, lbl = line.strip().split(\"\\t\")\n",
    "        samples.append(path)\n",
    "        labels.append(int(lbl))\n",
    "\n",
    "# === Grad-CAM è®¾ç½® ===\n",
    "target_layer = model.block8.branch1[-1]  # æœ€åå·ç§¯\n",
    "cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "# === æ¨ç† ===\n",
    "all_preds, all_probs, json_records = [], [], []\n",
    "\n",
    "for img_path, true_label in tqdm(zip(samples, labels), total=len(samples)):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        prob = torch.softmax(output, dim=1)[0, 1].item()\n",
    "        pred = output.argmax(dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_probs.append(prob)\n",
    "\n",
    "    # Grad-CAM\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(1)])[0, :]\n",
    "    rgb_img = transform(img).permute(1, 2, 0).cpu().numpy()\n",
    "    rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    out_name = os.path.basename(img_path).split(\".\")[0] + \"_gradcam.jpg\"\n",
    "    out_path = os.path.join(gradcam_dir, out_name)\n",
    "    Image.fromarray(cam_image).save(out_path)\n",
    "\n",
    "    json_records.append({\n",
    "        \"img\": img_path,\n",
    "        \"true\": int(true_label),\n",
    "        \"pred\": int(pred),\n",
    "        \"prob\": round(prob, 4),\n",
    "        \"correct\": int(pred == int(true_label))\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ… Grad-CAM saved to: {gradcam_dir}\")\n",
    "\n",
    "# === è®¡ç®—æŒ‡æ ‡ ===\n",
    "acc = accuracy_score(labels, all_preds)\n",
    "prec = precision_score(labels, all_preds)\n",
    "rec = recall_score(labels, all_preds)\n",
    "f1 = f1_score(labels, all_preds)\n",
    "try:\n",
    "    auc = roc_auc_score(labels, all_probs)\n",
    "except:\n",
    "    auc = float('nan')\n",
    "\n",
    "print(\"\\nğŸ¯ Evaluation Metrics:\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"AUROC:     {auc:.4f}\")\n",
    "\n",
    "# === ä¿å­˜ JSON ç»“æœ ===\n",
    "eval_dir = \"/data_lg/keru/project/part2/DermNet/pseudo_eval\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "metrics_path = os.path.join(eval_dir, \"pseudo_eval_metrics.json\")\n",
    "records_path = os.path.join(eval_dir, \"pseudo_eval_records.json\")\n",
    "\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(prec, 4),\n",
    "        \"Recall\": round(rec, 4),\n",
    "        \"F1 Score\": round(f1, 4),\n",
    "        \"AUROC\": round(auc, 4)\n",
    "    }, f, indent=4)\n",
    "\n",
    "with open(records_path, \"w\") as f:\n",
    "    json.dump(json_records, f, indent=4)\n",
    "\n",
    "print(f\"\\nğŸ“Š Metrics saved to: {metrics_path}\")\n",
    "print(f\"ğŸ“ Per-image results saved to: {records_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL TEST!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo test list saved to /data_lg/keru/project/part2/DermNet/eval/final_test_list.txt\n",
      "Acne: 312, Non-Acne: 3690\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# === é…ç½® ===\n",
    "train_root = \"/data_lg/keru/project/part2/DermNet/test\"\n",
    "output_txt = \"/data_lg/keru/project/part2/DermNet/eval/final_test_list.txt\"\n",
    "\n",
    "# === æ‰¾åˆ°æ‰€æœ‰å­æ–‡ä»¶å¤¹ ===\n",
    "all_classes = [\n",
    "    d for d in os.listdir(train_root)\n",
    "    if os.path.isdir(os.path.join(train_root, d))\n",
    "]\n",
    "\n",
    "# Acne ç±»å’Œ Non-Acne ç±»åˆ†å¼€\n",
    "acne_folders = [d for d in all_classes if \"acne\" in d.lower()]\n",
    "non_acne_folders = [d for d in all_classes if d not in acne_folders]\n",
    "\n",
    "assert acne_folders, \"æ²¡æœ‰æ‰¾åˆ°åŒ…å« 'acne' çš„æ–‡ä»¶å¤¹ï¼\"\n",
    "\n",
    "# === æ”¶é›†æ‰€æœ‰ Acne æ ·æœ¬ ===\n",
    "acne_samples = []\n",
    "for folder in acne_folders:\n",
    "    folder_path = os.path.join(train_root, folder)\n",
    "    imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    acne_samples.extend(imgs)\n",
    "\n",
    "# === æ”¶é›†æ‰€æœ‰ Non-Acne æ ·æœ¬ ===\n",
    "non_acne_samples = []\n",
    "for folder in non_acne_folders:\n",
    "    folder_path = os.path.join(train_root, folder)\n",
    "    imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    non_acne_samples.extend(imgs)\n",
    "\n",
    "# === åˆå¹¶ & ä¿å­˜ txt ===\n",
    "with open(output_txt, \"w\") as f:\n",
    "    for p in acne_samples:\n",
    "        f.write(f\"{p}\\t1\\n\")\n",
    "    for p in non_acne_samples:\n",
    "        f.write(f\"{p}\\t0\\n\")\n",
    "\n",
    "print(f\"Pseudo test list saved to {output_txt}\")\n",
    "print(f\"Acne: {len(acne_samples)}, Non-Acne: {len(non_acne_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4002/4002 [01:15<00:00, 53.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Grad-CAM saved to: /data_lg/keru/project/part2/DermNet/eval_real/eval_cyclegan\n",
      "   æ­£ç¡®æ ·æœ¬: 7 å¼ \n",
      "   é”™è¯¯æ ·æœ¬: 7 å¼ \n",
      "\n",
      "ğŸ¯ Evaluation Metrics:\n",
      "Accuracy:  0.6794\n",
      "Precision: 0.1100\n",
      "Recall:    0.4391\n",
      "F1 Score:  0.1760\n",
      "AUROC:     0.6206\n",
      "\n",
      "ğŸ“Š Metrics saved to: /data_lg/keru/project/part2/DermNet/eval_real/eval_cyclegan/eval_metrics.json\n",
      "ğŸ“ Per-image results saved to: /data_lg/keru/project/part2/DermNet/eval_real/eval_cyclegan/eval_records.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import json\n",
    "import random\n",
    "\n",
    "# === é…ç½® ===\n",
    "# pseudo_txt = \"/data_lg/keru/project/part2/DermNet/eval/final_test_list.txt\" #çœŸå®æ¯”ä¾‹\n",
    "pseudo_txt = \"/data_lg/keru/project/part2/DermNet/eval_real/final_test_list.txt\"\n",
    "model_path = \"/data_lg/keru/project/part2/output/output_cycleGAN/best_facenet_v2.pt\"  # â† æ”¹ä¸ºä½  v2/v3 æ¨¡å‹è·¯å¾„\n",
    "gradcam_dir = \"/data_lg/keru/project/part2/DermNet/eval_real/eval_cyclegan\"\n",
    "eval_dir = \"/data_lg/keru/project/part2/DermNet/eval_real/eval_cyclegan\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "os.makedirs(gradcam_dir, exist_ok=True)\n",
    "\n",
    "# === æ¨¡å‹åŠ è½½ï¼ˆå¤„ç† module.ï¼‰ ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=2)\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === é¢„å¤„ç† ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# === è¯»å–ä¼ªæµ‹è¯•é›†åˆ—è¡¨ ===\n",
    "samples, labels = [], []\n",
    "with open(pseudo_txt) as f:\n",
    "    for line in f:\n",
    "        path, lbl = line.strip().split(\"\\t\")\n",
    "        samples.append(path)\n",
    "        labels.append(int(lbl))\n",
    "\n",
    "# === Grad-CAM è®¾ç½® ===\n",
    "target_layer = model.block8.branch1[-1]  # æœ€åå·ç§¯\n",
    "cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "# === æ¨ç† ===\n",
    "all_preds, all_probs, json_records = [], [], []\n",
    "correct_samples = []\n",
    "wrong_samples = []\n",
    "\n",
    "for img_path, true_label in tqdm(zip(samples, labels), total=len(samples)):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        prob = torch.softmax(output, dim=1)[0, 1].item()\n",
    "        pred = output.argmax(dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_probs.append(prob)\n",
    "    is_correct = (pred == true_label)\n",
    "    \n",
    "    # ä¿å­˜æ ·æœ¬è·¯å¾„ã€é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾\n",
    "    sample_info = {\n",
    "        \"img_path\": img_path,\n",
    "        \"true_label\": true_label,\n",
    "        \"pred\": pred,\n",
    "        \"prob\": prob,\n",
    "        \"is_correct\": is_correct\n",
    "    }\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_samples.append(sample_info)\n",
    "    else:\n",
    "        wrong_samples.append(sample_info)\n",
    "\n",
    "    json_records.append({\n",
    "        \"img\": img_path,\n",
    "        \"true\": int(true_label),\n",
    "        \"pred\": int(pred),\n",
    "        \"prob\": round(prob, 4),\n",
    "        \"correct\": int(is_correct)\n",
    "    })\n",
    "\n",
    "# === éšæœºé€‰æ‹©7å¼ æ­£ç¡®å’Œ7å¼ é”™è¯¯çš„æ ·æœ¬ ===\n",
    "selected_correct = random.sample(correct_samples, min(7, len(correct_samples)))\n",
    "selected_wrong = random.sample(wrong_samples, min(7, len(wrong_samples)))\n",
    "selected_samples = selected_correct + selected_wrong\n",
    "\n",
    "# === ä¸ºé€‰ä¸­çš„æ ·æœ¬ç”Ÿæˆå¹¶ä¿å­˜Grad-CAM ===\n",
    "for i, sample in enumerate(selected_samples):\n",
    "    img = Image.open(sample[\"img_path\"]).convert(\"RGB\")\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # ç”ŸæˆGrad-CAM\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(1)])[0, :]\n",
    "    rgb_img = transform(img).permute(1, 2, 0).cpu().numpy()\n",
    "    rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "    \n",
    "    # ç¡®å®šè¾“å‡ºæ–‡ä»¶åå’Œè·¯å¾„\n",
    "    result_type = \"correct\" if sample[\"is_correct\"] else \"wrong\"\n",
    "    img_name = os.path.basename(sample[\"img_path\"]).split(\".\")[0]\n",
    "    out_name = f\"{result_type}_{i+1}_{img_name}_gradcam.jpg\"\n",
    "    out_path = os.path.join(gradcam_dir, out_name)\n",
    "    \n",
    "    # ä¿å­˜Grad-CAMå›¾åƒ\n",
    "    Image.fromarray(cam_image).save(out_path)\n",
    "\n",
    "print(f\"\\nâœ… Grad-CAM saved to: {gradcam_dir}\")\n",
    "print(f\"   æ­£ç¡®æ ·æœ¬: {len(selected_correct)} å¼ \")\n",
    "print(f\"   é”™è¯¯æ ·æœ¬: {len(selected_wrong)} å¼ \")\n",
    "\n",
    "# === è®¡ç®—æŒ‡æ ‡ ===\n",
    "acc = accuracy_score(labels, all_preds)\n",
    "prec = precision_score(labels, all_preds)\n",
    "rec = recall_score(labels, all_preds)\n",
    "f1 = f1_score(labels, all_preds)\n",
    "try:\n",
    "    auc = roc_auc_score(labels, all_probs)\n",
    "except:\n",
    "    auc = float('nan')\n",
    "\n",
    "print(\"\\nğŸ¯ Evaluation Metrics:\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"AUROC:     {auc:.4f}\")\n",
    "\n",
    "# === ä¿å­˜ JSON ç»“æœ ===\n",
    "\n",
    "\n",
    "metrics_path = os.path.join(eval_dir, \"eval_metrics.json\")\n",
    "records_path = os.path.join(eval_dir, \"eval_records.json\")\n",
    "\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(prec, 4),\n",
    "        \"Recall\": round(rec, 4),\n",
    "        \"F1 Score\": round(f1, 4),\n",
    "        \"AUROC\": round(auc, 4)\n",
    "    }, f, indent=4)\n",
    "\n",
    "with open(records_path, \"w\") as f:\n",
    "    json.dump(json_records, f, indent=4)\n",
    "\n",
    "print(f\"\\nğŸ“Š Metrics saved to: {metrics_path}\")\n",
    "print(f\"ğŸ“ Per-image results saved to: {records_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial ç‰ˆæœ¬è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm  # ä½¿ç”¨æ™®é€štqdmï¼Œä»…æ–‡æœ¬è¿›åº¦æ¡\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt  # ä»…ç”¨äºä¿å­˜å›¾è¡¨ï¼Œä¸æ˜¾ç¤º\n",
    "\n",
    "\n",
    "def build_model(name):\n",
    "    \"\"\"æ„å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹\"\"\"\n",
    "    if name == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    elif name == \"resnet50\":\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    elif name == \"efficientnet_b0\":\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {name}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"æ¨¡å‹éªŒè¯ä¸»å‡½æ•°ï¼ˆçº¯å‘½ä»¤è¡Œç‰ˆæœ¬ï¼‰\"\"\"\n",
    "    # === é…ç½®å‚æ•°ï¼ˆéœ€æ‰‹åŠ¨ä¿®æ”¹è·¯å¾„ï¼‰===\n",
    "    pseudo_txt = \"/data_lg/keru/project/part2/DermNet/eval/final_test_list.txt\"  # txtæµ‹è¯•é›†è·¯å¾„\n",
    "    model_path = \"/data_lg/keru/project/part2/outputs/best_resnet50.pt\"  # æ¨¡å‹è·¯å¾„\n",
    "    gradcam_dir = \"/data_lg/keru/project/part2/DermNet/eval/eval_initial/gram\"  # Grad-CAMä¿å­˜ç›®å½•\n",
    "    eval_dir = \"/data_lg/keru/project/part2/DermNet/eval/eval_initial\"  # è¯„ä¼°ç»“æœä¿å­˜ç›®å½•\n",
    "    batch_size = 32\n",
    "    model_name = \"resnet50\"\n",
    "\n",
    "    os.makedirs(gradcam_dir, exist_ok=True)\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "    # === æ¨¡å‹åŠ è½½ ===\n",
    "    model = build_model(model_name).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½è‡ª: {model_path}\")\n",
    "\n",
    "    # === æ•°æ®é¢„å¤„ç† ===\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # === ä»txtè¯»å–æµ‹è¯•é›† ===\n",
    "    samples, labels = [], []\n",
    "    with open(pseudo_txt) as f:\n",
    "        for line in f:\n",
    "            path, lbl = line.strip().split(\"\\t\")\n",
    "            samples.append(path)\n",
    "            labels.append(int(lbl))\n",
    "    print(f\"ğŸ“Š åŠ è½½ {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬\")\n",
    "\n",
    "    # === Grad-CAMè®¾ç½® ===\n",
    "    target_layer = model.layer4[-1] if 'resnet' in model_name else model.features[-1]\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "    # === æ¨ç†å¹¶æ”¶é›†ç»“æœ ===\n",
    "    all_preds, all_probs, json_records = [], [], []\n",
    "    correct_samples, wrong_samples = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(zip(samples, labels)), total=len(samples), desc=\"æ¨ç†ä¸­\")\n",
    "        for i, (img_path, true_label) in pbar:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = model(input_tensor)\n",
    "            prob = torch.softmax(outputs, dim=1)[:, 1].item()\n",
    "            pred = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "            all_preds.append(pred)\n",
    "            all_probs.append(prob)\n",
    "            is_correct = (pred == true_label)\n",
    "\n",
    "            # ä¿å­˜æ ·æœ¬ä¿¡æ¯\n",
    "            sample_info = {\n",
    "                \"img_path\": img_path,\n",
    "                \"true_label\": true_label,\n",
    "                \"pred\": pred,\n",
    "                \"prob\": prob,\n",
    "                \"is_correct\": is_correct\n",
    "            }\n",
    "\n",
    "            if is_correct:\n",
    "                correct_samples.append(sample_info)\n",
    "            else:\n",
    "                wrong_samples.append(sample_info)\n",
    "\n",
    "            json_records.append({\n",
    "                \"img\": img_path,\n",
    "                \"true\": int(true_label),\n",
    "                \"pred\": int(pred),\n",
    "                \"prob\": round(prob, 4),\n",
    "                \"correct\": int(is_correct)\n",
    "            })\n",
    "\n",
    "    # === éšæœºé€‰æ‹©7å¼ æ­£ç¡®å’Œ7å¼ é”™è¯¯æ ·æœ¬ ===\n",
    "    selected_correct = random.sample(correct_samples, min(7, len(correct_samples)))\n",
    "    selected_wrong = random.sample(wrong_samples, min(7, len(wrong_samples)))\n",
    "    selected_samples = selected_correct + selected_wrong\n",
    "    print(f\"ğŸ” é€‰ä¸­ {len(selected_correct)} æ­£ç¡®æ ·æœ¬å’Œ {len(selected_wrong)} é”™è¯¯æ ·æœ¬ç”ŸæˆGrad-CAM\")\n",
    "\n",
    "    # === ç”Ÿæˆå¹¶ä¿å­˜Grad-CAM ===\n",
    "    gradcam_results = []\n",
    "    for i, sample in enumerate(selected_samples):\n",
    "        img = Image.open(sample[\"img_path\"]).convert(\"RGB\")\n",
    "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # ç”ŸæˆGrad-CAM\n",
    "        target_class = sample[\"pred\"]\n",
    "        targets = [ClassifierOutputTarget(target_class)]\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "\n",
    "        rgb_image = input_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min())\n",
    "        visualization = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True)\n",
    "\n",
    "        # ä¿å­˜å›¾åƒ\n",
    "        result_type = \"correct\" if sample[\"is_correct\"] else \"wrong\"\n",
    "        img_name = os.path.basename(sample[\"img_path\"]).split(\".\")[0]\n",
    "        out_name = f\"{result_type}_{i + 1}_{img_name}_gradcam.jpg\"\n",
    "        out_path = os.path.join(gradcam_dir, out_name)\n",
    "        Image.fromarray(visualization).save(out_path)\n",
    "\n",
    "        # è®°å½•Grad-CAMä¿¡æ¯\n",
    "        gradcam_results.append({\n",
    "            \"image_path\": sample[\"img_path\"],\n",
    "            \"true_label\": sample[\"true_label\"],\n",
    "            \"predicted_class\": target_class,\n",
    "            \"probability\": sample[\"prob\"],\n",
    "            \"gradcam_path\": out_path,\n",
    "            \"is_correct\": sample[\"is_correct\"]\n",
    "        })\n",
    "\n",
    "        # ä»…æ–‡æœ¬æç¤ºï¼Œä¸æ˜¾ç¤ºå›¾åƒ\n",
    "        print(f\"\\nğŸ“¸ å·²ä¿å­˜ {result_type} æ ·æœ¬ {i+1} çš„Grad-CAM: {out_name}\")\n",
    "\n",
    "    # === è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ===\n",
    "    acc = accuracy_score(labels, all_preds)\n",
    "    prec = precision_score(labels, all_preds)\n",
    "    rec = recall_score(labels, all_preds)\n",
    "    f1 = f1_score(labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, all_probs)\n",
    "    except:\n",
    "        auc = float('nan')\n",
    "\n",
    "    print(f\"\\nğŸ¯ è¯„ä¼°æŒ‡æ ‡:\")\n",
    "    print(f\"  å‡†ç¡®ç‡: {acc:.4f}\")\n",
    "    print(f\"  ç²¾ç¡®ç‡: {prec:.4f}\")\n",
    "    print(f\"  å¬å›ç‡: {rec:.4f}\")\n",
    "    print(f\"  F1åˆ†æ•°: {f1:.4f}\")\n",
    "    print(f\"  AUROC: {auc:.4f}\")\n",
    "\n",
    "    # === ä¿å­˜JSONç»“æœ ===\n",
    "    metrics_path = os.path.join(eval_dir, \"eval_metrics.json\")\n",
    "    records_path = os.path.join(eval_dir, \"eval_records.json\")\n",
    "    gradcam_path = os.path.join(eval_dir, \"gradcam_summary.json\")\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"Accuracy\": round(acc, 4),\n",
    "            \"Precision\": round(prec, 4),\n",
    "            \"Recall\": round(rec, 4),\n",
    "            \"F1 Score\": round(f1, 4),\n",
    "            \"AUROC\": round(auc, 4)\n",
    "        }, f, indent=4)\n",
    "\n",
    "    with open(records_path, \"w\") as f:\n",
    "        json.dump(json_records, f, indent=4)\n",
    "\n",
    "    with open(gradcam_path, \"w\") as f:\n",
    "        json.dump(gradcam_results, f, indent=4)\n",
    "\n",
    "    print(f\"\\nğŸ“Š æŒ‡æ ‡ä¿å­˜è‡³: {metrics_path}\")\n",
    "    print(f\"ğŸ“ æ ·æœ¬è®°å½•ä¿å­˜è‡³: {records_path}\")\n",
    "    print(f\"ğŸ”¥ Grad-CAMæ±‡æ€»ä¿å­˜è‡³: {gradcam_path}\")\n",
    "\n",
    "    # === ç”ŸæˆæŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨ï¼ˆä»…ä¿å­˜ï¼Œä¸æ˜¾ç¤ºï¼‰===\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = [acc, prec, rec, f1, auc]\n",
    "    labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUROC\"]\n",
    "    plt.bar(labels, metrics, color=['skyblue', 'lightgreen', 'lightcoral', 'plum', 'lightyellow'])\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.title('æ¨¡å‹è¯„ä¼°æŒ‡æ ‡')\n",
    "    plt.ylabel('åˆ†æ•°')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    metrics_plot = os.path.join(eval_dir, \"metrics_barplot.png\")\n",
    "    plt.savefig(metrics_plot)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"ğŸ“ˆ æŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜è‡³: {metrics_plot}\")\n",
    "    print(\"ï¼ˆæ‰€æœ‰ç»“æœå¯åœ¨ä¿å­˜ç›®å½•ä¸­æŸ¥çœ‹ï¼‰\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1:1 positive & negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo test list saved to /data_lg/keru/project/part2/DermNet/eval/final_test_list_ratio.txt\n",
      "Acne: 312, Non-Acne: 312\n",
      "æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: 312:312 = 1:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# === é…ç½® ===\n",
    "train_root = \"/data_lg/keru/project/part2/DermNet/test\"\n",
    "output_txt = \"/data_lg/keru/project/part2/DermNet/eval/final_test_list_ratio.txt\"\n",
    "random_seed = 42  # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "random.seed(random_seed)\n",
    "\n",
    "# === æ‰¾åˆ°æ‰€æœ‰å­æ–‡ä»¶å¤¹ ===\n",
    "all_classes = [\n",
    "    d for d in os.listdir(train_root)\n",
    "    if os.path.isdir(os.path.join(train_root, d))\n",
    "]\n",
    "\n",
    "# Acne ç±»å’Œ Non-Acne ç±»åˆ†å¼€\n",
    "acne_folders = [d for d in all_classes if \"acne\" in d.lower()]\n",
    "non_acne_folders = [d for d in all_classes if d not in acne_folders]\n",
    "\n",
    "assert acne_folders, \"æ²¡æœ‰æ‰¾åˆ°åŒ…å« 'acne' çš„æ–‡ä»¶å¤¹ï¼\"\n",
    "\n",
    "# === æ”¶é›†æ‰€æœ‰ Acne æ ·æœ¬ ===\n",
    "acne_samples = []\n",
    "for folder in acne_folders:\n",
    "    folder_path = os.path.join(train_root, folder)\n",
    "    imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    acne_samples.extend(imgs)\n",
    "\n",
    "# === æ”¶é›†æ‰€æœ‰ Non-Acne æ ·æœ¬ ===\n",
    "non_acne_samples = []\n",
    "for folder in non_acne_folders:\n",
    "    folder_path = os.path.join(train_root, folder)\n",
    "    imgs = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    non_acne_samples.extend(imgs)\n",
    "\n",
    "# === éšæœºé€‰æ‹©ä¸Acneæ ·æœ¬æ•°é‡ç›¸åŒçš„Non-Acneæ ·æœ¬ ===\n",
    "num_acne = len(acne_samples)\n",
    "num_non_acne = len(non_acne_samples)\n",
    "\n",
    "if num_non_acne >= num_acne:\n",
    "    # å¦‚æœNon-Acneæ ·æœ¬æ•°é‡è¶³å¤Ÿï¼Œéšæœºé€‰æ‹©ä¸Acneç›¸åŒæ•°é‡çš„æ ·æœ¬\n",
    "    selected_non_acne = random.sample(non_acne_samples, num_acne)\n",
    "else:\n",
    "    # å¦‚æœNon-Acneæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨Non-Acneæ ·æœ¬å¹¶å¯¹å…¶è¿›è¡Œé‡å¤é‡‡æ ·\n",
    "    print(f\"è­¦å‘Š: Non-Acneæ ·æœ¬æ•°é‡({num_non_acne})å°‘äºAcneæ ·æœ¬æ•°é‡({num_acne})ï¼Œå°†è¿›è¡Œé‡å¤é‡‡æ ·\")\n",
    "    selected_non_acne = random.choices(non_acne_samples, k=num_acne)\n",
    "\n",
    "# === åˆå¹¶ & ä¿å­˜ txt ===\n",
    "with open(output_txt, \"w\") as f:\n",
    "    # å†™å…¥Acneæ ·æœ¬\n",
    "    for p in acne_samples:\n",
    "        f.write(f\"{p}\\t1\\n\")\n",
    "    # å†™å…¥éšæœºé€‰æ‹©çš„Non-Acneæ ·æœ¬\n",
    "    for p in selected_non_acne:\n",
    "        f.write(f\"{p}\\t0\\n\")\n",
    "\n",
    "print(f\"Pseudo test list saved to {output_txt}\")\n",
    "print(f\"Acne: {len(acne_samples)}, Non-Acne: {len(selected_non_acne)}\")\n",
    "print(f\"æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {len(acne_samples)}:{len(selected_non_acne)} = 1:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CFLD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
